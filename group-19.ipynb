{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1943632,"sourceType":"datasetVersion","datasetId":1108473}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EDA","metadata":{}},{"cell_type":"code","source":"# Dropping null values since dataset is huge\ndf= pd.read_csv('/kaggle/input/skillbuilder-data-2009-2010/2012-2013-data-with-predictions-4-final.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample the dataset (e.g., 10% of the data)\n# df = df.sample(frac=0.1, random_state=42)\n\n# Save the sampled dataset to a new CSV file\n# df.to_csv(\"sampled_dataset.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Structure of the dataset","metadata":{}},{"cell_type":"code","source":"# Setting plot style\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\nplt.figure(figsize=(12,6))\nsns.heatmap(df.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title(\"Missing Values in the Dataset\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Taking a sample Data set and verifying Heatmap of Missing values of Original dataset is similar to that of the sample dataset\nsample_df = df.sample(n=60000, random_state=42)\nplt.figure(figsize=(12,6))\nsns.heatmap(sample_df.isnull(), cmap=\"viridis\", cbar=False, yticklabels=False)\nplt.title(\"Missing Values in the Dataset - Sample Dataset\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-processing","metadata":{}},{"cell_type":"code","source":"# List of irrelevant columns\nirrelevant_columns = [\n    'problem_log_id', 'problem_id', 'user_id', 'teacher_id',\n    'school_id', 'answer_id', 'answer_text', 'actions', 'tutor_mode', 'skill'\n]\n\n# Drop them from the dataset\ndata = df.drop(columns=irrelevant_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = data.dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove any additional non-numeric characters from datetime strings\ndata['start_time'] = data['start_time'].str.split('.').str[0]\ndata['end_time'] = data['end_time'].str.split('.').str[0]\n\n# Then, convert to datetime\ndata['start_time'] = pd.to_datetime(data['start_time'], format=\"%Y-%m-%d %H:%M:%S\")\ndata['end_time'] = pd.to_datetime(data['end_time'], format=\"%Y-%m-%d %H:%M:%S\")\n\n# Calculate 'time_taken' in seconds\ndata['time_taken'] = (data['end_time'] - data['start_time']).dt.total_seconds()\n\n# Convert to float\ndata['time_taken'] = data['time_taken'].astype(float)\n\n# Drop the original datetime columns\ndata = data.drop(columns=['start_time', 'end_time'])\n\n# Check the result\nprint(data[['time_taken']].head())\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Identify categorical columns\ncategorical_columns = ['skill_id', 'problem_type', 'first_action', 'type']\n\n# Apply Label Encoding\nlabel_encoders = {}\nfor col in categorical_columns:\n    label_encoders[col] = LabelEncoder()\n    data[col] = label_encoders[col].fit_transform(data[col])\n\nprint(data.dtypes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming your dataset is `data` and your target column is 'correct'\nX = data.drop(columns=['correct'])  # Features\ny = data['correct']  # Target\ny = y.astype(int)\n\n# Handle missing values in features\nimputer = SimpleImputer(strategy='mean')  # Impute missing values with the mean\nX_imputed = imputer.fit_transform(X)  # Impute missing values\n\n# Convert imputed numpy array back to DataFrame\nX_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n\n# Stratified sampling to ensure equal class distribution\nX_sampled, X_test, y_sampled, y_test = train_test_split(X_imputed, y, test_size=0.99, stratify=y, random_state=42)\n\n# Train a Random Forest model to rank features\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_sampled, y_sampled)\n\n# Plot feature importance\nimportance = pd.Series(model.feature_importances_, index=X.columns)\nimportance.sort_values(ascending=False).plot(kind='bar', figsize=(10, 6), title='Feature Importance')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Assuming y_test and y_pred are already defined\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Optionally, visualize the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=set(y_test))\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training and hyper-parameter training","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n# Assuming your dataset is `data` and your target column is 'correct'\nX = data[['attempt_count', 'hint_count', 'bottom_hint', 'time_taken', 'first_action', 'ms_first_response', 'overlap_time',\n          'Average_confidence(CONCENTRATING)', 'Average_confidence(BORED)']] # Features\ny = data['correct']  # Target\ny = y.astype(int)\n\nimputer = SimpleImputer(strategy='most_frequent')\nX_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n\n# Stratified sampling to ensure equal class distribution\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.99, stratify=y, random_state=42)\n\n# Impute missing values in both training and test sets\nresults = []\ny_preds = dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nimport warnings\n\n# Define models and hyperparameter grids for grid search\nmodels = {\n    'Linear Regression': LinearRegression(),\n    # 'Polynomial Regression': Pipeline([\n    #     ('poly', PolynomialFeatures()),\n    #     ('linear', LinearRegression())\n    # ]),\n    'K-Nearest Neighbors': KNeighborsClassifier(),\n    'Support Vector Machine': SVC(),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42)\n}\n\nparam_grids = {\n    'Linear Regression': {},  # No hyperparameters for basic linear regression\n    'Polynomial Regression': {\n        'poly__degree': [2, 3, 4]  # Testing polynomial degrees\n    },\n    'K-Nearest Neighbors': {\n        'n_neighbors': [3, 5, 7, 9],\n        'weights': ['uniform', 'distance']\n    },\n    'Support Vector Machine': {\n        'C': [0.1, 1, 10],\n        'kernel': ['linear', 'rbf', 'poly']\n    },\n    'Random Forest': {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [None, 10, 20, 30]\n    },\n    'Decision Tree': {\n        'max_depth': [None, 5, 10, 15],\n        'min_samples_split': [2, 5, 10]\n    }\n}\n\n# Placeholder for result\nbest_params = None\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f\"Training {model_name}...\")\n    if param_grids[model_name]:  # If hyperparameters are defined\n        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring='f1_weighted', cv=3, n_jobs=-1, verbose = 3)\n        grid_search.fit(X_train, y_train)\n        best_model = grid_search.best_estimator_\n        best_params = grid_search.best_params_\n    else:\n        best_model = model.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = best_model.predict(X_test)\n    # print(\"y_pred before : \", y_pred.dtype)\n    y_preds[model_name] = y_pred\n\n    if y_pred.dtype != int:\n      \n      y_pred = (y_pred > 0.5).astype(int)\n    # print(\"y_pred after : \", y_pred.dtype)\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    print(f\"Model:{model_name}, Accuracy: {accuracy}, F1 Score: {f1}, Best Parameters: {best_params}\")\n    results.append({'Model': model_name, 'Accuracy': accuracy, 'F1 Score': f1, 'Best Parameters': best_params})\n\n# Convert results to a DataFrame for display\nresults_df = pd.DataFrame(results) \n\n# Display results\nprint(results_df.sort_values(by='F1 Score', ascending=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_params","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Recommendation System","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\n\n# Load the dataset\n# df = pd.read_csv(\"skill_builder_data.csv\")  # Replace with the actual file path\n\n# Step 1: Identify weak skills for each student\n# Filter out problems that were answered incorrectly\nincorrect_answers = df[df['correct'] == 0]\n\n# Group by `user_id` and aggregate their weak `skill_id`\nstudent_weak_skills = incorrect_answers.groupby('user_id')['skill_id'].apply(\n    lambda x: ', '.join(map(str, x.dropna().unique()))\n).reset_index()\nstudent_weak_skills.rename(columns={'skill_id': 'weak_skills'}, inplace=True)\n\n# Step 2: Prepare the resources (problems with skill associations)\nresource_skills = df[['problem_id', 'skill_id']].drop_duplicates()\nresource_skills['skill_id'] = resource_skills['skill_id'].astype(str)\n\n# Step 3: Combine skills data for TF-IDF vectorization\ncombined_skills = pd.concat([\n    student_weak_skills['weak_skills'],\n    resource_skills['skill_id']\n], axis=0).fillna('')\n\n# Vectorize skill data using TF-IDF with limited dimensions\nvectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\nskills_tfidf = vectorizer.fit_transform(combined_skills)\n\n# Step 4: Compute similarity using sparse operations\n# Split back into student and resource vectors\nstudent_skills_tfidf = skills_tfidf[:len(student_weak_skills)]\nresource_skills_tfidf = skills_tfidf[len(student_weak_skills):]\n\n# Function to compute similarity for a single student using sparse operations\ndef compute_similarity(student_vector, resource_vectors, top_n=5):\n    similarities = cosine_similarity(student_vector, resource_vectors).flatten()\n    top_indices = similarities.argsort()[-top_n:][::-1]\n    return top_indices, similarities[top_indices]\n\n# Step 5: Create a recommendation function\ndef recommend_resources(student_id, top_n=5):\n    # Check if the student exists in the weak_skills data\n    if student_id not in student_weak_skills['user_id'].values:\n        print(f\"Student ID {student_id} not found.\")\n        return pd.DataFrame()\n\n    # Get the student's index in the TF-IDF matrix\n    student_index = student_weak_skills[student_weak_skills['user_id'] == student_id].index[0]\n    student_vector = student_skills_tfidf[student_index]\n\n    # Compute similarity for this student only\n    top_resource_indices, similarity_scores = compute_similarity(\n        student_vector, resource_skills_tfidf, top_n=top_n\n    )\n\n    # Get the top recommended resources\n    recommended_resources = resource_skills.iloc[top_resource_indices].copy()\n    recommended_resources['similarity_score'] = similarity_scores\n\n    return recommended_resources\n\n# Step 6: Test the recommender system\nstudent_id = 52535  # Replace with an actual `user_id` from the dataset\nrecommendations = recommend_resources(student_id, top_n=5)\n\n# Display recommendations\nif recommendations.empty:\n    print(f\"No recommendations found for student ID {student_id}.\")\nelse:\n    print(recommendations[['problem_id', 'skill_id', 'similarity_score']])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming a dictionary mapping skill_id to skill_name\nskill_mapping = {359.0: \"Algebra\", 393.0: \"Trignometry\"}  # Ensure this dictionary includes all relevant skill IDs\n\n# Ensure matching data types\nrecommendations['skill_id'] = recommendations['skill_id'].astype(float)\n\n# Make a copy of the recommendations DataFrame to avoid SettingWithCopyWarning\nrecommendations = recommendations.copy()\n\n# Ensure matching data types\nrecommendations.loc[:, 'skill_id'] = recommendations['skill_id'].astype(float)\n\n# Map skill IDs and handle missing mappings\nrecommendations.loc[:, 'skill_name'] = recommendations['skill_id'].map(skill_mapping).fillna(\"Unknown Skill\")\n\n# Display recommendations with skill names\nprint(recommendations[['problem_id', 'skill_name']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
